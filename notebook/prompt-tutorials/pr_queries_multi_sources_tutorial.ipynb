{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import dotenv\n",
    "import simplejson as json\n",
    "import polars as pl\n",
    "import json_repair\n",
    "from tqdm import tqdm\n",
    "from more_itertools import chunked\n",
    "from loguru import logger\n",
    "import asyncio as asio\n",
    "from justatom.tooling.stl import reuuid\n",
    "from justatom.storing.dataset import API as DatasetApi\n",
    "\n",
    "from justatom.etc.io import io_snapshot\n",
    "from justatom.tooling.reqs import openai_chat\n",
    "from justatom.tooling.coro import _limit_concurrency\n",
    "from justatom.running.igni import IGNIRunner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def source_from_dataset(dataset_name_or_path, **props):\n",
    "    from justatom.storing.dataset import API as DatasetApi\n",
    "    import polars as pl\n",
    "\n",
    "    maybe_df_or_iter = DatasetApi.named(dataset_name_or_path).iterator(**props)\n",
    "    if isinstance(maybe_df_or_iter, pl.DataFrame):\n",
    "        pl_data = maybe_df_or_iter\n",
    "    else:\n",
    "        dataset = list(maybe_df_or_iter)\n",
    "        pl_data = pl.from_dicts(dataset)\n",
    "    return pl_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrapper_for_props(d: dict, must_include_keys: list[str] = None) -> dict:\n",
    "    \"\"\"\n",
    "    :param d: Source doc\n",
    "    :param must_include_keys: List of keys to include\n",
    "    :return: New doc with only specified `must_include_keys`\n",
    "    \"\"\"\n",
    "    must_include_keys = d.keys() if must_include_keys is None else must_include_keys\n",
    "    return {key: d[key] for key in must_include_keys if key in d}\n",
    "\n",
    "def wrapper_json_decode(pl_docs: pl.DataFrame, columns: list[str], must_include_keys: list[str] = None) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    :param pl.DataFrame: Source documents, usually coming `pl_docs.from pl.DataFrame.to_dicts()`.\n",
    "    :param must_include_keys: List of keys to include from source `js_docs`.\n",
    "    :return: New Polars.DataFrame with `json_decode` on the given `columns` where each row is a list of string.\n",
    "    \"\"\"\n",
    "    js_docs = pl_docs.to_dicts()\n",
    "    \n",
    "    js_answer= [\n",
    "        {\n",
    "            **{col: json_repair.loads(js_doc[col]) for col in columns},\n",
    "            **wrapper_for_props(js_doc, must_include_keys=[\"doc_id\", \"doc_name\"])\n",
    "        } for js_doc in js_docs\n",
    "    ]\n",
    "    \n",
    "    return pl.from_dicts(js_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def source_from_dataset(dataset_name_or_path, **props):\n",
    "    from justatom.storing.dataset import API as DatasetApi\n",
    "    import polars as pl\n",
    "\n",
    "    maybe_df_or_iter = DatasetApi.named(dataset_name_or_path).iterator(**props)\n",
    "    if isinstance(maybe_df_or_iter, pl.DataFrame):\n",
    "        pl_data = maybe_df_or_iter\n",
    "    else:\n",
    "        dataset = list(maybe_df_or_iter)\n",
    "        pl_data = pl.from_dicts(dataset)\n",
    "    return pl_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name_or_path = Path.home() / \"IDataset\" / \"SEVERSTAL\" / \"SEVERSTAL_48_split_by_line.xlsx\"\n",
    "\n",
    "content_col = \"chunk_text\"\n",
    "title_col = \"doc_name\"\n",
    "chunk_id_col = \"chunk_id\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_source_docs = source_from_dataset(dataset_name_or_path, sheet_name=\"Chunks\")\n",
    "pl_meta_docs = source_from_dataset(dataset_name_or_path, sheet_name=\"Documents\")\n",
    "pl_meta_docs = wrapper_json_decode(pl_meta_docs, columns=[\"doc_chunk_ids\", \"doc_sections\"], must_include_keys=[\"doc_id\", \"doc_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_source_docs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(f\"There are N=[{pl_source_docs.select('chunk_id').unique().shape[0]}] unique chunks\")\n",
    "logger.info(f\"There are D=[{pl_meta_docs.select('doc_id').unique().shape[0]}] unique docs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def pipeline(\n",
    "    js_docs: list[dict],\n",
    "    pr_runner,\n",
    "    openai_model_name: str,\n",
    "    batch_size: int = 16,\n",
    "    coros_size: int = 2,\n",
    "    save_snapshot_every: int = 5,\n",
    "    snapshot_prefix: str = None,\n",
    "    snapshot_where: str = None,\n",
    "    timeout: int = 512,\n",
    "    must_include_keys: list[str] | None = None,\n",
    "    validate_json_response: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    We process `js_docs` by chunks where each chunk is of size `batch_size`.\n",
    "    Each chunk is processed asynchronously via parallel `coros_size` coroutines.\n",
    "\n",
    "    :param js_docs: documents to process\n",
    "    :param pr_runner: One of the instance `IPromptRunner` to create specific prompt\n",
    "    \"\"\"\n",
    "    pipes = []\n",
    "\n",
    "    for i, batch in tqdm(enumerate(chunked(js_docs, n=batch_size))):\n",
    "        _batch = batch\n",
    "        cur_result = await asio.gather(\n",
    "            *_limit_concurrency(\n",
    "                [\n",
    "                    openai_chat(\n",
    "                        pr_runner.prompt(**d),\n",
    "                        timeout=timeout,\n",
    "                        model=openai_model_name,\n",
    "                        props=wrapper_for_props(d, must_include_keys=must_include_keys),\n",
    "                    )\n",
    "                    for d in _batch\n",
    "                ],\n",
    "                concurrency=coros_size,\n",
    "            )\n",
    "        )\n",
    "        if not validate_json_response:\n",
    "            pipes.extend(cur_result)\n",
    "        else:\n",
    "            # NOTE: Order of execution is preserved.\n",
    "            js_answer_docs = [\n",
    "                pr_runner.finalize(\n",
    "                    raw_response=js_res[\"response\"], **wrapper_for_props(js_doc, must_include_keys=must_include_keys)\n",
    "                )\n",
    "                for js_doc, js_res in zip(batch, cur_result, strict=True)\n",
    "            ]\n",
    "            pipes.extend(js_answer_docs)\n",
    "\n",
    "        if (i + 1) % save_snapshot_every == 0:\n",
    "            io_snapshot(pipes, where=snapshot_where, snapshot_number=str(i + 1), snapshot_prefix=snapshot_prefix)\n",
    "    return pipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_source_docs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_meta_docs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrapper_for_paragraphs(paragraphs: list[str]):\n",
    "    content = \"\\n\".join([f\"Paragraph {pos}: {text}\" for pos, text in enumerate(paragraphs)])\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_docs = pl_meta_docs\\\n",
    "    .explode(\"doc_chunk_ids\")\\\n",
    "    .join(\n",
    "        pl_source_docs,\n",
    "        how=\"left\",\n",
    "        left_on=[\"doc_id\", \"doc_chunk_ids\"],  # doc_id + текущий chunk_id\n",
    "        right_on=[\"doc_id\", \"chunk_id\"]\n",
    "    ).groupby([\"doc_id\", \"doc_name\"]).agg([\n",
    "        pl.col(\"doc_chunk_ids\").flatten().alias(\"doc_chunk_ids\"),\n",
    "        pl.col(\"doc_sections\").flatten().alias(\"doc_sections\"),\n",
    "        pl.col(content_col).explode().map_elements(lambda paragraphs: wrapper_for_paragraphs(paragraphs)).alias(\"content\")\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(f\"There are total G=[{len(pl_docs)}] groups of chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = f\"\"\"\n",
    "Generate synthetic data from short questions that users can ask chatbots or customer support in Russian.\n",
    "\n",
    "The goal is to anticipate possible user questions, ensuring that they are clearly worded, appropriate to the context, and can be easily answered based on the context.\n",
    "\n",
    "# Stages of creating synthetic data\n",
    "1. Carefully study the document submitted to you and its context.\n",
    "2. Formulate brief questions related to the document with multiple paragraphs and make sure that there are clear answers in the text.\n",
    "3. Make sure that each question is unique, and use different formulations to ensure diversity.\n",
    "4. Formulate answers strictly based on the content (paragraphs) of the document.\n",
    "\n",
    "(Optional: for real-world examples, more complex documents may be required, as well as various pairs of quality tests. Use PLACEHOLDERS for real texts and quality control.)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from justatom.running.prompt import QUERIESWithSourcesPromptRunner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# openai_model_name = \"gpt-4o-mini\"\n",
    "openai_model_name = \"o3-mini\"\n",
    "batch_size = 4\n",
    "coros_size = 2\n",
    "save_snapshot_every = 1\n",
    "must_include_keys = [\"doc_id\", content_col, title_col]\n",
    "snapshot_prefix = \"SEVERSTAL|QueriesWithSources\"\n",
    "snapshot_where = \"outputs\"\n",
    "source_language=\"Russian\"\n",
    "timeout  = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_runner = QUERIESWithSourcesPromptRunner(\n",
    "    source_language=source_language,\n",
    "    system_prompt=system_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_runner.system_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_view_docs = pl_docs.select([\"doc_id\", \"content\", \"doc_chunk_ids\", \"doc_name\"])\n",
    "pl_view_docs = pl_view_docs.rename({title_col: \"title\"})\n",
    "js_view_docs = pl_view_docs.to_dicts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_runner._prepare(\n",
    "    **js_view_docs[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_runner.prompt(**js_view_docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = await pipeline(js_view_docs,openai_model_name=openai_model_name, pr_runner=pr_runner, batch_size=batch_size, coros_size=coros_size, save_snapshot_every=save_snapshot_every, must_include_keys=must_include_keys, snapshot_prefix=snapshot_prefix, snapshot_where=snapshot_where, timeout=timeout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_docs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Merge` results\n",
    "\n",
    "Now, after the annotation, let's take the latest file—or the one we want to check—from the `outputs` directory, where the `LLM` synthetic annotation results were periodically saved. We'll load it from there and merge it with the correct paragraphs by `chunk_id` according to `doc_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_response = source_from_dataset(Path(os.getcwd()) / \"outputs\" / \"SEVERSTAL|QueriesWithSources3.json\")\n",
    "js_response = pl_response.to_dicts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "js_response_struct = [{\n",
    "    **json_repair.loads(js_doc['response']),\n",
    "    **wrapper_for_props(js_doc, must_include_keys=[\"doc_id\"])\n",
    "    } for js_doc in js_response[:6]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "js_response_struct[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_response_struct = pl.from_dicts(js_response_struct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_response_struct.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_final_response = pl_response_struct.join(pl_docs, on=\"doc_id\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_final_response.explode(\"answer\").rename({\"answer\": \"QA\"}).write_excel(f\"SEVERSTAL.{openai_model_name}.QueriesWithMultiSources.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "justatom",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
