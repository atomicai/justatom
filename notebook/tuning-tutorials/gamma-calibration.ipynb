{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# io\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Iterable\n",
    "from collections import Counter\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "from more_itertools import chunked\n",
    "import math\n",
    "# standard\n",
    "import dotenv\n",
    "import simplejson as json\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import pytorch_lightning as L\n",
    "import torch\n",
    "from transformers import PreTrainedTokenizer, PreTrainedTokenizerFast\n",
    "from loguru import logger\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset\n",
    "# justatom\n",
    "from justatom.logging.io import CSVLogger\n",
    "from justatom.processing.sample import Sample, SampleBasket\n",
    "from justatom.logging.wandb import WandbLogger\n",
    "from justatom.tooling import stl\n",
    "from justatom.modeling.mask import ILanguageModel\n",
    "from justatom.processing import IProcessor, ITokenizer, igniset, RuntimeProcessor, TrainWithContrastiveProcessor\n",
    "from justatom.processing.loader import NamedDataLoader\n",
    "from justatom.running.encoders import EncoderRunner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from justatom.tooling.dataset import DatasetRecordAdapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maybe_cuda_or_mps():\n",
    "    if torch.cuda.is_available():\n",
    "        return \"cuda:0\"\n",
    "    elif torch.has_mps:\n",
    "        return \"mps\"\n",
    "    else:\n",
    "        return \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = Path(os.getcwd()) / \".data\" / \"polaroids.ai.data.json\"\n",
    "num_samples = 100  # debug subset size\n",
    "\n",
    "adapter = DatasetRecordAdapter.from_source(\n",
    "    dataset_path,\n",
    "    content_col=\"content\",\n",
    "    queries_col=\"queries\",\n",
    "    chunk_id_col=\"chunk_id\",\n",
    "    keywords_col=\"keywords_or_phrases\",\n",
    "    preserve_all_fields=False,\n",
    " )\n",
    "\n",
    "docs = list(adapter.iterator())\n",
    "docs_df = pl.from_dicts(docs)\n",
    "\n",
    "pl_data = (\n",
    "    docs_df\n",
    "    .with_columns(\n",
    "        queries=pl.col(\"meta\").struct.field(\"labels\"),\n",
    "        chunk_id=pl.col(\"id\"),\n",
    "        keywords_or_phrases=pl.col(\"meta\").struct.field(\"keywords_or_phrases\"),\n",
    "    )\n",
    "    .select([\"content\", \"queries\", \"chunk_id\", \"keywords_or_phrases\"] )\n",
    "    .explode(\"queries\")\n",
    "    .filter(\n",
    "        pl.col(\"queries\").is_not_null()\n",
    "        & (pl.col(\"queries\").cast(pl.Utf8).str.len_chars() > 0)\n",
    "    )\n",
    "    .with_columns(queries=pl.col(\"queries\").cast(pl.Utf8))\n",
    "    .sample(shuffle=True, fraction=1.0)\n",
    "    .head(num_samples)\n",
    " )\n",
    "\n",
    "js_data = pl_data.to_dicts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pl_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>queries</th><th>content</th></tr><tr><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;Hey fellow nerds, can anyone e…</td><td>&quot;(Grima): &quot;The wizard had three…</td></tr><tr><td>&quot;In the universe of &#x27;Harry Pott…</td><td>&quot;(Hagrid): &quot;Alright. Let&#x27;s go.&quot;…</td></tr><tr><td>&quot;Hey gamers! In the Harry Potte…</td><td>&quot;No one on Privet Drive had eve…</td></tr><tr><td>&quot;What is the feared weapon used…</td><td>&quot;Lupin&#x27;s eyebrows crept up in s…</td></tr><tr><td>&quot;What were the dynamics and out…</td><td>&quot;Adelaide Ivanovna, immediately…</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 2)\n",
       "┌─────────────────────────────────┬─────────────────────────────────┐\n",
       "│ queries                         ┆ content                         │\n",
       "│ ---                             ┆ ---                             │\n",
       "│ str                             ┆ str                             │\n",
       "╞═════════════════════════════════╪═════════════════════════════════╡\n",
       "│ Hey fellow nerds, can anyone e… ┆ (Grima): \"The wizard had three… │\n",
       "│ In the universe of 'Harry Pott… ┆ (Hagrid): \"Alright. Let's go.\"… │\n",
       "│ Hey gamers! In the Harry Potte… ┆ No one on Privet Drive had eve… │\n",
       "│ What is the feared weapon used… ┆ Lupin's eyebrows crept up in s… │\n",
       "│ What were the dynamics and out… ┆ Adelaide Ivanovna, immediately… │\n",
       "└─────────────────────────────────┴─────────────────────────────────┘"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl_data.select([\"queries\", \"content\"]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl_data.select(\"content\").unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content': '(Grima): \"The wizard had three companions. An Elf, a dwarf, and a human\".\\n(Saruman): \"You smell like a horse. Was the man from Gondor?\"\\n(Grima): \"No. From the north. I think, one of the Ranger of the North. Dressed poorly, and also... he wears a strange ring. Two snakes with emerald eyes. One consuming the other, crowned with golden flowers\".\\n(Saruman): \"The ring of Barahir. Gandalf the Grey thinks that he found Isildur\\'s Heir - the lost king of Gondor. What a fool. Their line ended many years ago. But it doesn\\'t matter. The world of men will fall anyway and it will start with Edoras\".',\n",
       " 'queries': \"Hey fellow nerds, can anyone explain why Saruman dismisses the possibility of a surviving line from Gondor's kings despite the clues about the ring of Barahir presented by Grima? Let's dissect the lore! #MiddleEarthMysteries\",\n",
       " 'chunk_id': '918d8c4a-4e0c-5d95-ac21-07746b59a465',\n",
       " 'keywords_or_phrases': [{'keyword_or_phrase': 'Ranger of the North',\n",
       "   'explanation': \"A group of skilled warriors who protect the northern lands in the world of 'The Lord of the Rings.'\"},\n",
       "  {'keyword_or_phrase': 'ring of Barahir',\n",
       "   'explanation': \"A special ring in 'The Lord of the Rings' that is a symbol of an ancient and noble family.\"},\n",
       "  {'keyword_or_phrase': \"Isildur's Heir\",\n",
       "   'explanation': 'Refers to the descendant of Isildur, a legendary king, suggesting that his family line continues.'},\n",
       "  {'keyword_or_phrase': 'the world of men',\n",
       "   'explanation': \"In 'The Lord of the Rings,' this phrase means the human societies and their territories.\"},\n",
       "  {'keyword_or_phrase': 'will fall',\n",
       "   'explanation': 'Means that the human kingdoms will be defeated or come to an end.'},\n",
       "  {'keyword_or_phrase': 'Edoras',\n",
       "   'explanation': \"The capital city of Rohan, an important location in 'The Lord of the Rings.'\"}]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "js_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-02-17 07:45:35.176\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mjustatom.modeling.mask\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m149\u001b[0m - \u001b[1mLoading from huggingface hub via \"intfloat/multilingual-e5-base\"\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model_name_or_path = \"intfloat/multilingual-e5-base\"\n",
    "tokenizer = ITokenizer.from_pretrained(model_name_or_path)\n",
    "# processor = RuntimeProcessor(tokenizer=tokenizer, max_seq_len=512)\n",
    "lm_model = ILanguageModel.load(model_name_or_path=model_name_or_path)\n",
    "processor = TrainWithContrastiveProcessor(tokenizer=tokenizer, max_seq_len=512, queries_field=\"queries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'queries'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.queries_field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'content'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.pos_queries_field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "device = maybe_cuda_or_mps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, tensor_names, _, baskets = processor.dataset_from_dicts(js_data, return_baskets=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = NamedDataLoader(dataset=dataset, tensor_names=tensor_names, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"query: What is the feared weapon used by dementors to destroy their victims in the 'Harry Potter and the Prisoner of Azkaban'?\",\n",
       " \"passage: Lupin's eyebrows crept up in surprise. 'Ron and Hermione brought me from Hogsmeade,' Harry lied without blinking an eye. 'Ah,' Lupin drawled. But there was still a moment of suspicion in his look. 'Well, let's toast to the victory of Gryffindor over Slytherin! But, of course, as a teacher, I shouldn't prefer any house,' he hastily added. They were drinking lemonade in silence, but Harry had one question on his tongue. 'What's under a dementor's hood?' he finally blurted out. Professor Lupin, detaching himself from the bottle, frowned. 'You see, those few who know it are not able to tell about it. The thing is that dementors pull back the hood only to use their final and most frightening weapon... 'What weapon?' 'It's called the Dementor’s Kiss,' Lupin said, grimacing. 'Dementors use it on those they want to completely destroy. I think they have something like a mouth under the hood, they press their jaws to the mouth of the victim and suck out their soul.\"]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baskets[3].samples[0].clear_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-02-17 07:45:42.345\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m1\u001b[0m - \u001b[1mtorch.Size([4, 512])\u001b[0m\n",
      "\u001b[32m2026-02-17 07:45:42.346\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m2\u001b[0m - \u001b[1mtorch.Size([4, 512])\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "logger.info(next(iter(loader))['pos_input_ids'].shape) # content# batch_size x max_seq_len\n",
    "logger.info(next(iter(loader))[\"input_ids\"].shape) # queries # batch_size x max_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"query: In the universe of 'Harry Potter and the Philosopher's Stone', what discovery did Hagrid make a few weeks prior to taking Harry and others into the Forbidden Forest?\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.tokenizer.decode(next(iter(loader))['input_ids'][1].squeeze(), skip_special_tokens=True, clean_up_tokenization_spaces=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-02-17 07:45:47.866\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mjustatom.running.encoders\u001b[0m:\u001b[36mto\u001b[0m:\u001b[36m36\u001b[0m - \u001b[1mMoving to device cuda:0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "lm_runner = EncoderRunner(\n",
    "    model=lm_model,\n",
    "    processor=processor,\n",
    "    prediction_heads=[],\n",
    "    device=device\n",
    ").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "278043648"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in lm_runner.model.eval().parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _BaseGammaTrainer(nn.Module):\n",
    "    def __init__(self, lm_runner, device: str = \"cpu\", stopsyms: str | None = None, freeze_encoder: bool = True):\n",
    "        super().__init__()\n",
    "        self.runner = lm_runner\n",
    "        self.device = device\n",
    "        self.processor = lm_runner.processor\n",
    "        self.freeze_encoder = freeze_encoder\n",
    "        self.stopsyms = \"«»:\\\"'\" if stopsyms is None else stopsyms\n",
    "        self._configure_encoder()\n",
    "        self.runner.to(device)\n",
    "\n",
    "    def _configure_encoder(self):\n",
    "        if self.freeze_encoder:\n",
    "            self.runner.eval()\n",
    "            for tensor in self.runner.parameters():\n",
    "                tensor.requires_grad = False\n",
    "        else:\n",
    "            self.runner.train()\n",
    "            for tensor in self.runner.parameters():\n",
    "                tensor.requires_grad = True\n",
    "\n",
    "    def _fn_inverse_idf_recall(self, query: str, keywords_or_phrases_or_content: list[str] | str, stopsyms: str | None = None):\n",
    "        stopsyms = stopsyms or self.stopsyms\n",
    "        stopsyms = string.punctuation if stopsyms is None else stopsyms + string.punctuation\n",
    "        if isinstance(keywords_or_phrases_or_content, list):\n",
    "            k_words = Counter(\n",
    "                stl.flatten_list([\n",
    "                    \"\".join([w for w in kwp.lower().strip() if w not in stopsyms]).split()\n",
    "                    for kwp in keywords_or_phrases_or_content\n",
    "                ])\n",
    "            )\n",
    "        else:\n",
    "            k_words = Counter([\n",
    "                \"\".join([ch for ch in w.lower().strip() if ch not in stopsyms])\n",
    "                for w in keywords_or_phrases_or_content.split()\n",
    "            ])\n",
    "        q_words = \"\".join(w for w in query if w not in stopsyms).lower().strip().split()\n",
    "        idf_recall = sum([1.0 / math.log(1 + k_words.get(w, 1)) for w in q_words if w in k_words]) / sum(\n",
    "            [1.0 / math.log(1 + k_words.get(w, 1)) for w in q_words]\n",
    "        )\n",
    "        return idf_recall\n",
    "\n",
    "    def _encode(self, batch):\n",
    "        if self.freeze_encoder:\n",
    "            with torch.no_grad():\n",
    "                return self.runner(batch, average=True, norm=True)\n",
    "        return self.runner(batch, average=True, norm=True)\n",
    "\n",
    "    def _build_rank_matrix(self, batch, shape):\n",
    "        rank_matrix = torch.zeros(shape, device=self.device, requires_grad=False)\n",
    "        with torch.no_grad():\n",
    "            for i, q_tokens in enumerate(batch[\"input_ids\"]):\n",
    "                for j, d_tokens in enumerate(batch[\"pos_input_ids\"]):\n",
    "                    queries = self.processor.tokenizer.decode(\n",
    "                        q_tokens,\n",
    "                        skip_special_tokens=True,\n",
    "                        clean_up_tokenization_spaces=True,\n",
    "                    )[len(self.processor.queries_prefix):].strip()\n",
    "                    content = self.processor.tokenizer.decode(\n",
    "                        d_tokens,\n",
    "                        skip_special_tokens=True,\n",
    "                        clean_up_tokenization_spaces=True,\n",
    "                    )[len(self.processor.pos_queries_prefix):].strip()\n",
    "                    rank_matrix[i, j] = self._fn_inverse_idf_recall(queries, content)\n",
    "        return rank_matrix\n",
    "\n",
    "    def _grad_norm(self, parameters) -> float:\n",
    "        grads = [p.grad.detach().float().norm(2) for p in parameters if p.grad is not None]\n",
    "        if not grads:\n",
    "            return 0.0\n",
    "        return torch.stack(grads).norm(2).item()\n",
    "\n",
    "    def gamma_parameters(self) -> list[nn.Parameter]:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def gamma_metrics(self) -> dict[str, float]:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def gamma_grad_metrics(self) -> dict[str, float]:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def mix_scores(self, scores, rank_matrix):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def build_optimizer(self, lr_gamma: float = 1e-2, lr_encoder: float = 2e-5, weight_decay: float = 0.01):\n",
    "        param_groups = [{\"params\": self.gamma_parameters(), \"lr\": lr_gamma, \"weight_decay\": 0.0}]\n",
    "        if not self.freeze_encoder:\n",
    "            encoder_params = [p for p in self.runner.parameters() if p.requires_grad]\n",
    "            if encoder_params:\n",
    "                param_groups.append({\n",
    "                    \"params\": encoder_params,\n",
    "                    \"lr\": lr_encoder,\n",
    "                    \"weight_decay\": weight_decay,\n",
    "                })\n",
    "        return optim.AdamW(param_groups)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        batch = {k: batch[k].to(self.device) for k in batch}\n",
    "        q_vecs, d_vecs = self._encode(batch)\n",
    "        scores = q_vecs @ d_vecs.T\n",
    "        rank_matrix = self._build_rank_matrix(batch, scores.shape)\n",
    "        return self.mix_scores(scores, rank_matrix)\n",
    "\n",
    "    def train(self, loader: NamedDataLoader, optimizer, logger=None, n_epochs: int = 1, save_dir: str | Path = None):\n",
    "        for epoch_idx in range(n_epochs):\n",
    "            for _, batch in tqdm(enumerate(loader)):\n",
    "                output = self.forward(batch)\n",
    "                labels = torch.arange(len(output), device=self.device)\n",
    "                loss = F.cross_entropy(output, labels)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "\n",
    "                grad_metrics = self.gamma_grad_metrics()\n",
    "                grad_metrics[\"Grad_Norm_model\"] = self._grad_norm(self.runner.parameters())\n",
    "\n",
    "                optimizer.step()\n",
    "                if logger is not None:\n",
    "                    logger.log_metrics({\n",
    "                        \"Loss\": loss.item(),\n",
    "                        \"FreezeEncoder\": int(self.freeze_encoder),\n",
    "                        **self.gamma_metrics(),\n",
    "                        **grad_metrics,\n",
    "                    })\n",
    "            if save_dir is not None:\n",
    "                save_path = Path(save_dir) / self.save_subdir / f\"epoch{str(epoch_idx + 1)}\"\n",
    "                self.runner.save(save_path)\n",
    "\n",
    "\n",
    "class BiGAMMATrainer(_BaseGammaTrainer):\n",
    "    save_subdir = \"BiGamma\"\n",
    "\n",
    "    def __init__(self, lm_runner, device: str = \"cpu\", stopsyms: str | None = None, freeze_encoder: bool = True):\n",
    "        super().__init__(lm_runner=lm_runner, device=device, stopsyms=stopsyms, freeze_encoder=freeze_encoder)\n",
    "        self.gamma1 = nn.Parameter(torch.tensor([0.5], device=device), requires_grad=True)\n",
    "        self.gamma2 = nn.Parameter(torch.tensor([1.5], device=device), requires_grad=True)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def gamma_parameters(self) -> list[nn.Parameter]:\n",
    "        return [self.gamma1, self.gamma2]\n",
    "\n",
    "    def gamma_metrics(self) -> dict[str, float]:\n",
    "        return {\"Gamma1\": self.gamma1.item(), \"Gamma2\": self.gamma2.item()}\n",
    "\n",
    "    def gamma_grad_metrics(self) -> dict[str, float]:\n",
    "        return {\n",
    "            \"Grad_norm_gamma1\": self._grad_norm([self.gamma1]),\n",
    "            \"Grad_norm_gamma2\": self._grad_norm([self.gamma2]),\n",
    "        }\n",
    "\n",
    "    def mix_scores(self, scores, rank_matrix):\n",
    "        gamma1_ = self.sigmoid(self.gamma1)\n",
    "        gamma2_ = self.sigmoid(self.gamma2)\n",
    "        return gamma1_ * scores + gamma2_ * rank_matrix\n",
    "\n",
    "\n",
    "class GAMMATrainer(_BaseGammaTrainer):\n",
    "    save_subdir = \"Gamma\"\n",
    "\n",
    "    def __init__(self, lm_runner, device: str = \"cpu\", stopsyms: str | None = None, freeze_encoder: bool = True):\n",
    "        super().__init__(lm_runner=lm_runner, device=device, stopsyms=stopsyms, freeze_encoder=freeze_encoder)\n",
    "        self.gamma = nn.Parameter(torch.tensor([0.5], device=device), requires_grad=True)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def gamma_parameters(self) -> list[nn.Parameter]:\n",
    "        return [self.gamma]\n",
    "\n",
    "    def gamma_metrics(self) -> dict[str, float]:\n",
    "        return {\"Gamma\": self.gamma.item()}\n",
    "\n",
    "    def gamma_grad_metrics(self) -> dict[str, float]:\n",
    "        return {\n",
    "            \"Grad_norm_gamma1\": self._grad_norm([self.gamma]),\n",
    "            \"Grad_norm_gamma2\": 0.0,\n",
    "        }\n",
    "\n",
    "    def mix_scores(self, scores, rank_matrix):\n",
    "        gamma_ = self.sigmoid(self.gamma)\n",
    "        return gamma_ * scores + (1 - gamma_) * rank_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-02-17 07:46:01.072\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mjustatom.running.encoders\u001b[0m:\u001b[36mto\u001b[0m:\u001b[36m36\u001b[0m - \u001b[1mMoving to device cuda:0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "experiment_freeze_encoder = True  # True: tune only gammas | False: end2end encoder+gammas\n",
    "trainer = BiGAMMATrainer(\n",
    "    lm_runner=lm_runner,\n",
    "    device=device,\n",
    "    freeze_encoder=experiment_freeze_encoder,\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = trainer.build_optimizer(\n",
    "    lr_gamma=1e-2,\n",
    "    lr_encoder=2e-5,\n",
    "    weight_decay=0.01,\n",
    " )\n",
    "\n",
    "wb_logger = CSVLogger(Path(os.getcwd()) / \"weights\" / \"debug_bi_gamma_metrics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import gc\n",
    "\n",
    "def _to_gb(num_bytes: int) -> float:\n",
    "    return num_bytes / (1024 ** 3)\n",
    "\n",
    "def cuda_mem_report(tag: str, reset_peak: bool = False):\n",
    "    if not torch.cuda.is_available():\n",
    "        print(f\"[{tag}] CUDA is not available\")\n",
    "        return\n",
    "    if reset_peak:\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "    allocated = torch.cuda.memory_allocated()\n",
    "    reserved = torch.cuda.memory_reserved()\n",
    "    max_allocated = torch.cuda.max_memory_allocated()\n",
    "    max_reserved = torch.cuda.max_memory_reserved()\n",
    "    print(\n",
    "        f\"[{tag}] alloc={_to_gb(allocated):.2f}GB | reserved={_to_gb(reserved):.2f}GB | \"\n",
    "        f\"peak_alloc={_to_gb(max_allocated):.2f}GB | peak_reserved={_to_gb(max_reserved):.2f}GB\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BiGamma BEFORE] alloc=1.04GB | reserved=1.09GB | peak_alloc=1.04GB | peak_reserved=1.09GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:02, 11.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BiGamma AFTER] alloc=1.04GB | reserved=1.20GB | peak_alloc=1.12GB | peak_reserved=1.20GB\n"
     ]
    }
   ],
   "source": [
    "cuda_mem_report(\"BiGamma BEFORE\", reset_peak=True)\n",
    "trainer.train(loader, optimizer=optimizer, logger=wb_logger, n_epochs=1, save_dir=Path(os.getcwd()) / \"weights\")\n",
    "cuda_mem_report(\"BiGamma AFTER\", reset_peak=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_mem_report(\"After cleanup\", reset_peak=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-02-17 07:49:01.849\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mjustatom.running.encoders\u001b[0m:\u001b[36mto\u001b[0m:\u001b[36m36\u001b[0m - \u001b[1mMoving to device cuda:0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "experiment_freeze_encoder = False  # compare against frozen setup\n",
    "trainer = GAMMATrainer(\n",
    "    lm_runner=lm_runner,\n",
    "    device=device,\n",
    "    freeze_encoder=experiment_freeze_encoder,\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = trainer.build_optimizer(\n",
    "    lr_gamma=1e-2,\n",
    "    lr_encoder=2e-5,\n",
    "    weight_decay=0.01,\n",
    " )\n",
    "wb_logger = CSVLogger(Path(os.getcwd()) / \"weights\" / \"gamma_metrics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gamma BEFORE] alloc=1.04GB | reserved=1.20GB | peak_alloc=1.04GB | peak_reserved=1.20GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:05,  4.87it/s]\n"
     ]
    }
   ],
   "source": [
    "cuda_mem_report(\"Gamma BEFORE\", reset_peak=True)\n",
    "trainer.train(loader, optimizer=optimizer, logger=wb_logger, n_epochs=1, save_dir=Path(os.getcwd()) / \"weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_mem_report(\"Gamma AFTER\", reset_peak=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if wb_logger is not None:\n",
    "    wb_logger.close_log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jarvis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
