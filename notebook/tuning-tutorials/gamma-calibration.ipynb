{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/justatom/ISpell/envs/justatom/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# io\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Iterable\n",
    "from collections import Counter\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "from more_itertools import chunked\n",
    "import math\n",
    "# standard\n",
    "import dotenv\n",
    "import simplejson as json\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import pytorch_lightning as L\n",
    "import torch\n",
    "from transformers import PreTrainedTokenizer, PreTrainedTokenizerFast\n",
    "from loguru import logger\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset\n",
    "# justatom\n",
    "from justatom.processing.sample import Sample, SampleBasket\n",
    "from justatom.logging.wandb import WandbLogger\n",
    "from justatom.tooling import stl\n",
    "from justatom.modeling.mask import ILanguageModel\n",
    "from justatom.processing import IProcessor, ITokenizer, igniset, INFERProcessor, ContrastiveProcessor\n",
    "from justatom.processing.loader import NamedDataLoader\n",
    "from justatom.running.m1 import M1LMRunner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def source_from_dataset(dataset_name_or_path, **props):\n",
    "    from justatom.storing.dataset import API as DatasetApi\n",
    "    import polars as pl\n",
    "\n",
    "    maybe_df_or_iter = DatasetApi.named(dataset_name_or_path).iterator(**props)\n",
    "    if isinstance(maybe_df_or_iter, pl.DataFrame):\n",
    "        pl_data = maybe_df_or_iter\n",
    "    else:\n",
    "        dataset = list(maybe_df_or_iter)\n",
    "        pl_data = pl.from_dicts(dataset)\n",
    "    return pl_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maybe_cuda_or_mps():\n",
    "    if torch.cuda.is_available():\n",
    "        return \"cuda:0\"\n",
    "    elif torch.has_mps:\n",
    "        return \"mps\"\n",
    "    else:\n",
    "        return \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_data = source_from_dataset(Path(os.getcwd()) / \".data\" / \"polaroids.ai.data.all.json\").select([\"content\", \"queries\", \"chunk_id\", \"keywords_or_phrases\"]).explode(\"queries\").filter(pl.col(\"queries\") != None).sample(shuffle=True, fraction=1.0)\n",
    "js_data = pl_data.to_dicts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pl_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_data.select([\"queries\", \"content\"]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_data.select(\"content\").unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "js_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = \"intfloat/multilingual-e5-base\"\n",
    "tokenizer = ITokenizer.from_pretrained(model_name_or_path)\n",
    "# processor = INFERProcessor(tokenizer=tokenizer, max_seq_len=512)\n",
    "lm_model = ILanguageModel.load(model_name_or_path=model_name_or_path)\n",
    "processor = ContrastiveProcessor(tokenizer=tokenizer, max_seq_len=512, queries_field=\"queries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.queries_field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.pos_queries_field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "device = maybe_cuda_or_mps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, tensor_names, _, baskets = processor.dataset_from_dicts(js_data, return_baskets=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = NamedDataLoader(dataset=dataset, tensor_names=tensor_names, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baskets[3].samples[0].clear_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(next(iter(loader))['pos_input_ids'].shape) # content# batch_size x max_seq_len\n",
    "logger.info(next(iter(loader))[\"input_ids\"].shape) # queries # batch_size x max_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.tokenizer.decode(next(iter(loader))['input_ids'][1].squeeze(), skip_special_tokens=True, clean_up_tokenization_spaces=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_runner = M1LMRunner(\n",
    "    model=lm_model,\n",
    "    processor=processor,\n",
    "    prediction_heads=[],\n",
    "    device=device\n",
    ").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(p.numel() for p in lm_runner.model.eval().parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiGAMMATrainer(nn.Module):\n",
    "    \n",
    "    def __init__(self, lm_runner, device: str = \"cpu\", stopsyms: str | None = None):\n",
    "        super().__init__()\n",
    "        self.gamma1 = nn.Parameter(torch.Tensor([0.5]).to(device), requires_grad=True)\n",
    "        self.gamma2 = nn.Parameter(torch.Tensor([1.5]).to(device), requires_grad=True)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.runner = lm_runner.eval()\n",
    "        self.device = device\n",
    "        self.processor = lm_runner.processor\n",
    "        self.stopsyms = \"«»:\\\"'\" if stopsyms is None else stopsyms\n",
    "        \n",
    "        for name, tensor in self.runner.named_parameters():\n",
    "            tensor.requires_grad=False\n",
    "        self.runner.training=False\n",
    "        \n",
    "        self.runner.to(device)\n",
    "    \n",
    "    def wrapper_for_keywords_or_content(self, js_doc, include_keywords: bool = False, include_explanation: bool = False, include_content: bool = False):\n",
    "        if not include_content and not include_keywords and not include_explanation:\n",
    "            raise ValueError(f\"You selected [include_keywords=False][include_content=False][include_explanation=False]\")\n",
    "        keywords_or_phrases = js_doc.get(\"keywords_or_phrases\", [])\n",
    "        keywords_content: str = [js_doc['content']] if include_content else []\n",
    "        if include_keywords and include_explanation:\n",
    "            keywords_content += [\n",
    "                kwp[\"keyword_or_phrase\"].strip() + \" \" + kwp[\"explanation\"].strip() for kwp in keywords_or_phrases\n",
    "            ]\n",
    "        elif include_keywords:\n",
    "            keywords_content += [kwp[\"keyword_or_phrase\"].strip() for kwp in keywords_or_phrases]\n",
    "        else:\n",
    "            keywords_content += [kwp[\"explanation\"].strip() for kwp in keywords_or_phrases]\n",
    "            keywords_content += \"\\n\".join([kwp[\"explanation\"].strip() for kwp in keywords_or_phrases])\n",
    "        return keywords_content\n",
    "\n",
    "    def _fn_inverse_idf_recall(self, query: str, keywords_or_phrases_or_content: list[str] | str, stopsyms: str | None = None, **props):\n",
    "        stopsyms = stopsyms or self.stopsyms\n",
    "        stopsyms = string.punctuation if stopsyms is None else stopsyms + string.punctuation\n",
    "        if isinstance(keywords_or_phrases_or_content, list):\n",
    "            k_words = Counter(stl.flatten_list([\"\".join([w for w in kwp.lower().strip() if w not in stopsyms]).split() for kwp in keywords_or_phrases_or_content]))\n",
    "        else:\n",
    "            k_words = Counter([\"\".join([ch for ch in w.lower().strip() if ch not in stopsyms]) for w in keywords_or_phrases_or_content.split()])\n",
    "        q_words = \"\".join(w for w in query if w not in stopsyms).lower().strip().split()\n",
    "        idf_recall = sum([1.0 / math.log(1 + k_words.get(w, 1)) for w in q_words if w in k_words]) / sum(\n",
    "            [1.0 / math.log(1 + k_words.get(w, 1)) for w in q_words]\n",
    "        )\n",
    "        return idf_recall\n",
    "    \n",
    "    def forward(self, batch):\n",
    "        batch = {k:batch[k].to(self.device) for k in batch}\n",
    "        q_vecs, d_vecs = lm_runner(batch, average=True, norm=True)\n",
    "        scores = q_vecs @ d_vecs.T\n",
    "        R = torch.zeros((scores.shape[0], scores.shape[1]), device=self.device, requires_grad=False)\n",
    "        with torch.no_grad():\n",
    "            for i, q_tokens in enumerate(batch[\"input_ids\"]):\n",
    "                for j, d_tokens in enumerate(batch[\"pos_input_ids\"]):\n",
    "                    queries = self.processor.tokenizer.decode(q_tokens, skip_special_tokens=True, clean_up_tokenization_spaces=True)[len(self.processor.queries_prefix):].strip()\n",
    "                    content = self.processor.tokenizer.decode(d_tokens, skip_special_tokens=True, clean_up_tokenization_spaces=True)[len(self.processor.pos_queries_prefix):].strip()\n",
    "                    rank = self._fn_inverse_idf_recall(queries, content)\n",
    "                    try:\n",
    "                        R[i, j] = rank\n",
    "                    except IndexError:\n",
    "                        logger.info(f\"Error @ batch for tokens=[{str(i)}, {str(j)}]\")\n",
    "                        return batch\n",
    "        gamma1_ = self.sigmoid(self.gamma1)\n",
    "        gamma2_ = self.sigmoid(self.gamma2)\n",
    "        output = gamma1_ * scores + gamma2_ * R\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def train(self, loader: NamedDataLoader, optimizer, logger = None, n_epochs: int = 1):\n",
    "        for epoch_idx, _ in enumerate(range(n_epochs)):\n",
    "            for batch_idx, batch in tqdm(enumerate(loader)):\n",
    "                output = self.forward(batch) # batch_size x batch_size\n",
    "                labels = torch.arange(len(output), device=self.device)\n",
    "                loss = F.cross_entropy(output, labels)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                if logger is not None:\n",
    "                    logger.log_metrics({\n",
    "                        \"Loss\": loss.item(),\n",
    "                        \"Gamma1\": self.gamma1.item(),\n",
    "                        \"Gamma2\": self.gamma2.item()\n",
    "                    })\n",
    "            _save_dir = Path(save_dir) / \"BiGamma\" / f\"epoch{str(epoch_idx + 1)}\"\n",
    "            self.runner.save(_save_dir)\n",
    "        _save_dir = Path(save_dir) / \"BiGamma\" / f\"epoch{str(epoch_idx + 1)}\"\n",
    "        self.runner.save(_save_dir)\n",
    "            \n",
    "            \n",
    "class GAMMATrainer(nn.Module):\n",
    "    \n",
    "    def __init__(self, lm_runner, device: str = \"cpu\", stopsyms: str | None = None):\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(torch.Tensor([0.5]).to(device), requires_grad=True)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.runner = lm_runner.eval()\n",
    "        self.device = device\n",
    "        self.processor = lm_runner.processor\n",
    "        self.stopsyms = \"«»:\\\"'\" if stopsyms is None else stopsyms\n",
    "        \n",
    "        for name, tensor in self.runner.named_parameters():\n",
    "            tensor.requires_grad=False\n",
    "        self.runner.training=False\n",
    "        \n",
    "        self.runner.to(device)\n",
    "    \n",
    "    def wrapper_for_keywords_or_content(self, js_doc, include_keywords: bool = False, include_explanation: bool = False, include_content: bool = False):\n",
    "        if not include_content and not include_keywords and not include_explanation:\n",
    "            raise ValueError(f\"You selected [include_keywords=False][include_content=False][include_explanation=False]\")\n",
    "        keywords_or_phrases = js_doc.get(\"keywords_or_phrases\", [])\n",
    "        keywords_content: str = [js_doc['content']] if include_content else []\n",
    "        if include_keywords and include_explanation:\n",
    "            keywords_content += [\n",
    "                kwp[\"keyword_or_phrase\"].strip() + \" \" + kwp[\"explanation\"].strip() for kwp in keywords_or_phrases\n",
    "            ]\n",
    "        elif include_keywords:\n",
    "            keywords_content += [kwp[\"keyword_or_phrase\"].strip() for kwp in keywords_or_phrases]\n",
    "        else:\n",
    "            keywords_content += [kwp[\"explanation\"].strip() for kwp in keywords_or_phrases]\n",
    "            keywords_content += \"\\n\".join([kwp[\"explanation\"].strip() for kwp in keywords_or_phrases])\n",
    "        return keywords_content\n",
    "    \n",
    "    def _fn_inverse_idf_recall(self, query: str, keywords_or_phrases_or_content: list[str] | str, stopsyms: str | None = None, **props):\n",
    "        stopsyms = stopsyms or self.stopsyms\n",
    "        stopsyms = string.punctuation if stopsyms is None else stopsyms + string.punctuation\n",
    "        if isinstance(keywords_or_phrases_or_content, list):\n",
    "            k_words = Counter(stl.flatten_list([\"\".join([w for w in kwp.lower().strip() if w not in stopsyms]).split() for kwp in keywords_or_phrases_or_content]))\n",
    "        else:\n",
    "            k_words = Counter([\"\".join([ch for ch in w.lower().strip() if ch not in stopsyms]) for w in keywords_or_phrases_or_content.split()])\n",
    "        q_words = \"\".join(w for w in query if w not in stopsyms).lower().strip().split()\n",
    "        idf_recall = sum([1.0 / math.log(1 + k_words.get(w, 1)) for w in q_words if w in k_words]) / sum(\n",
    "            [1.0 / math.log(1 + k_words.get(w, 1)) for w in q_words]\n",
    "        )\n",
    "        return idf_recall\n",
    "    \n",
    "    def forward(self, batch):\n",
    "        batch = {k:batch[k].to(self.device) for k in batch}\n",
    "        q_vecs, d_vecs = lm_runner(batch, average=True, norm=True)\n",
    "        scores = q_vecs @ d_vecs.T\n",
    "        R = torch.zeros((scores.shape[0], scores.shape[1]), device=self.device, requires_grad=False)\n",
    "        with torch.no_grad():\n",
    "            for i, q_tokens in enumerate(batch[\"input_ids\"]):\n",
    "                for j, d_tokens in enumerate(batch[\"pos_input_ids\"]):\n",
    "                    queries = self.processor.tokenizer.decode(q_tokens, skip_special_tokens=True, clean_up_tokenization_spaces=True)[len(self.processor.queries_prefix):].strip()\n",
    "                    content = self.processor.tokenizer.decode(d_tokens, skip_special_tokens=True, clean_up_tokenization_spaces=True)[len(self.processor.pos_queries_prefix):].strip()\n",
    "                    rank = self._fn_inverse_idf_recall(queries, content)\n",
    "                    try:\n",
    "                        R[i, j] = rank\n",
    "                    except IndexError:\n",
    "                        logger.info(f\"Error @ batch for tokens=[{str(i)}, {str(j)}]\")\n",
    "                        return batch\n",
    "        gamma_ = self.sigmoid(self.gamma)\n",
    "        output = gamma_ * scores + (1 - gamma_) * R\n",
    "\n",
    "        return output\n",
    "\n",
    "    def train(self, loader: NamedDataLoader, optimizer, logger = None, n_epochs: int = 1, save_dir: str | Path = None):\n",
    "        for epoch_idx, _ in enumerate(range(n_epochs)):\n",
    "            for batch_idx, batch in tqdm(enumerate(loader)):\n",
    "                output = self.forward(batch) # batch_size x batch_size\n",
    "                labels = torch.arange(len(output), device=self.device)\n",
    "                loss = F.cross_entropy(output, labels)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                if logger is not None:\n",
    "                    logger.log_metrics({\n",
    "                        \"Loss\": loss.item(),\n",
    "                        \"Gamma\": self.gamma.item(),\n",
    "                    })\n",
    "            _save_dir = Path(save_dir) / \"Gamma\" / f\"epoch{str(epoch_idx + 1)}\"\n",
    "            self.runner.save(_save_dir)\n",
    "        _save_dir = Path(save_dir) / \"Gamma\" / f\"epoch{str(epoch_idx + 1)}\"\n",
    "        self.runner.save(_save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = BiGAMMATrainer(lm_runner=lm_runner, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW([trainer.gamma1, trainer.gamma2])\n",
    "\n",
    "wb_logger = WandbLogger(project=\"justatom.ai\", name=\"BiGamma AdamW descent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(loader, optimizer=optimizer, logger=wb_logger, n_epochs=2, save_dir=Path(os.getcwd()) / \"weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = GAMMATrainer(lm_runner = lm_runner, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW([trainer.gamma])\n",
    "wb_logger = WandbLogger(project=\"justatom.ai\", name=\"Gamma AdamW descent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(loader, optimizer=optimizer, logger=wb_logger, n_epochs=2, save_dir=Path(os.getcwd()) / \"weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wb_logger.close_log()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "justatom",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
