{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from loguru import logger\n",
    "from pathlib import Path\n",
    "import os\n",
    "import uuid\n",
    "from typing import Generator\n",
    "import numpy as np\n",
    "import simplejson as json\n",
    "import torch\n",
    "from justatom.tooling.dataset import source_from_dataset\n",
    "from justatom.etc.schema import Document\n",
    "from more_itertools import chunked\n",
    "import json_repair\n",
    "import polars as pl\n",
    "\n",
    "from justatom.storing.weaviate import Finder as WeaviateApi\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✔️ANN document store backed by <a href=\"https://github.com/weaviate/weaviate\">weaviate</a>\n",
    "\n",
    "> First, let'c make sure you have docker up and running. From the root of directory run:\n",
    "```bash\n",
    "docker-compose up -d\n",
    "```\n",
    "\n",
    "❗️ By default weavaite will run on port `2211`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_name = \"HelloWorld\"\n",
    "weaviate_host, weaviate_port = \"localhost\", 2211"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store = WeaviateApi.find(collection_name, WEAVIATE_HOST=weaviate_host, WEAVIATE_PORT=weaviate_port)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(f\"For the collection=[{collection_name}] you have N=[{store.count_documents()}] documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✔️ Prepare datasets\n",
    "\n",
    "> For this tutorial we will use built-in dataset `polaroids.ai`. This is the dataset from movies, games and books containing paragraphs from various moments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name_or_path = Path(os.getcwd()) / \".data\" / \"polaroids.ai.data.all.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_docs = source_from_dataset(dataset_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(f\"Columns=[{' | '.join(pl_docs.columns)}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">❗️Please do note, that `content` and `id` columns are must have. They describe each \"chunk\". All the rest fields are optional and would be added to `meta`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have `chunk_id` but not `id`. Let's add it as well.\n",
    "\n",
    "pl_docs = pl_docs.with_columns([\n",
    "    pl.col(\"chunk_id\").alias(\"id\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ❗️Let's filter out those chunks having `null` on any of \"must-have\" columns otherwise pipeline will fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_docs = pl_docs.filter((pl.col(\"content\") != None) & (pl.col(\"id\") != None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(f\"There are D=[{pl_docs.shape[0]}] unique documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">❗️We would like to keep `keywords_or_phrases` and relevant `queries` for each chunk. Let's declare that as well as original `chunk_id` to keep the structure outside of weaviate internal generated UUID-s.\n",
    "\n",
    "> ❗️❗️ Each chunk is associated with an array of relevant queries to describe it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "|  queries (list[str])  |     content: str     |   chunk_id: str   |\n",
    "|:---------------------:|:--------------------:|:-----------------:|\n",
    "| 1. ...thinking about 'The Hunger Games' mechanics, if you were in the same shoes as Gale, entering your name forty-two times to feed your fam, how would you strategize your game in the actual Arena? Would you team up or go solo based on these high stakes? <br><br>2.In the universe of 'The Hunger Games', what are tesserae and what do they offer to the participants in the Harvest?    | And here's where the real interest begins. Suppose you're poor and starving. Then you can ask to be included in the Harvest more times than you're entitled to, and in return you'd get tesserae. They give you grain and oil for a whole year for one tessera per person. You won't be full, but it's better than nothing. You can take tesserae for the whole family. When I was twelve, my name was entered four times. Once by law, and once more for tesserae for Prim, my mother, and myself. The next years had to do the same. And since the price of a tessera increases by one entry each year, now that I've turned sixteen, my name will be on twenty cards. Gale is eighteen, and he's been feeding a family of five for seven years. His name will be entered forty two times! It's clear that people like Madge, who has never had to risk because of tesserae, annoy Gale. Next to us, the inhabitants of the slag heap, she simply has no chance of getting into the games. Well, almost no chance. Of course, the rules are set by the Capitol, not the districts, let alone Madge's relatives, and it's still hard to sympathize with those who, like you, don't have to trade their own skin for a piece of bread.  | 80504cd8-9b21-514c-b001-4761d8c71044         |\n",
    "|-----------------------|----------------------|-------------------|\n",
    "| 1.In 'Harry Potter and the Philosopher's Stone', what misconception had Harry and Hermione initially had about Snape's intentions before learning the truth? <br><br>2. Hey peeps, why is Harry all jittery and pacing around the room even after telling Hermione about the whole Snape and Voldemort situation?        | Ron was asleep in the common room - apparently, he had been waiting for their return and had dozed off unnoticed. When Harry roughly shook him, Ron began to yell something about breaking the rules of a game, as if he were dreaming about a Quidditch match. However, after a few seconds, Ron completely woke up and, with his eyes wide open, listened to the story of Hermione and Harry. Harry was so excited that he could not sit still and paced back and forth across the room, trying to stay as close to the fireplace as possible. He was still shaking with cold. 'Snape wants to steal the stone for Voldemort. And Voldemort is waiting in the forest... And all this time we thought Snape wanted to steal the stone to become rich... And Voldemort...'  | 5ad25a92-28d9-5971-a81b-4f795898eeab         |\n",
    "|-----------------------|----------------------|-------------------|\n",
    "| 1. Hey fellow gamers, in The Hunger Games universe, if you were in a match where your ally was taken down first like Rue, how would you strategize your next move to survive against top opponents like Cato?<br><br> 2. In the 'Hunger Games' novel, why does Cato decide to spare Katniss's life after their encounter?    | What was she babbling about? You're Rue's ally? - I... I... we teamed up. We blew up the food of the Pros. I wanted to save her. Really did. But he found her first, the guy from District One - I say. Perhaps if Cato knows I helped Rue, he will kill me quickly and painlessly. - Did you kill him? - he asks grimly. - Yes. I killed him. And I covered her body with flowers. I sang to her till she fell asleep. Tears well up in my eyes. Will and strength are leaving me. There's only Rue, the pain in my head, fear of Cato and the moan of the dying girl. - Fell asleep? - mocks Cato. - Died. I sang to her till she died - I say. - Your district... sent me bread. I raise my hand - not for an arrow; I won't have time anyway. I just blow my nose. - Cato, make it quick, okay? His face shows conflicting emotions. Cato puts down the rock and says with almost a reproach: - This time, only this time, I'm letting you go. For the girl. We are even. No one owes anything to anyone anymore, understand? I nod, because I do understand. Understand about debts. About how bad it is to have them. Understand that if Cato wins, he will return to a district that has forgotten the rules to thank me. And Cato is neglecting them, too. Right now, he's not going to crack my head with a stone.  | b317200c-7fd3-5804-bbe4-bff33432ad0e         |\n",
    "|-----------------------|----------------------|-------------------|\n",
    "\n",
    "</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_include = [\n",
    "    \"keywords_or_phrases\",\n",
    "    \"chunk_id\",\n",
    "    \"queries\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrapper_for_docs(\n",
    "    pl_data: pl.DataFrame,\n",
    "    content_field: str,\n",
    "    keywords_or_phrases_field: str = None,\n",
    "    batch_size: int = 128,\n",
    "    columns_to_include: list[str] | None = None,\n",
    "    filters: dict | None = None,\n",
    "):\n",
    "    js_data = pl_data.to_dicts()\n",
    "    for js_chunk in tqdm(js_data):\n",
    "        js_meta = {k: js_chunk[k] for k in columns_to_include}\n",
    "        yield dict(content=js_chunk[content_field], meta=js_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "js_docs = list(wrapper_for_docs(\n",
    "    pl_docs,\n",
    "     content_field=\"content\",\n",
    "     columns_to_include=columns_to_include\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling\n",
    "\n",
    "> See <a href=\"https://huggingface.co/intfloat/multilingual-e5-large\">E5 large</a> , <a href=\"https://huggingface.co/intfloat/multilingual-e5-base\">E5 base</a>, <a href=\"https://huggingface.co/intfloat/multilingual-e5-small\">E5 small</a> family of encoder models. More coming soon\n",
    " \n",
    "> 📎 <a href=\"https://arxiv.org/abs/2212.03533\">paper</a>\n",
    "\n",
    "> ❗️For this tutorial we pick the base one `intfloat/multilingual-e5-base` as a trade-off between performance and precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = \"intfloat/multilingual-e5-base\"\n",
    "\n",
    "from justatom.modeling.mask import ILanguageModel\n",
    "from justatom.running.m1 import M1LMRunner\n",
    "from justatom.processing import INFERProcessor, ITokenizer\n",
    "lm_model = ILanguageModel.load(model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maybe_cuda_or_mps():\n",
    "    if torch.backends.mps.is_built():\n",
    "        return \"mps\"\n",
    "    elif torch.cuda.is_available():\n",
    "        return \"cuda:0\"\n",
    "    else:\n",
    "        return \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = maybe_cuda_or_mps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner = M1LMRunner(model=lm_model, prediction_heads=[], device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = INFERProcessor(ITokenizer.from_pretrained(model_name_or_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❗️According to the <a href=\"https://arxiv.org/abs/2212.03533\">paper</a> E5 family is trained in assymetric way meaning:\n",
    "\n",
    "> Use `\"query: \"` and `\"passage: \"` correspondingly for asymmetric tasks such as passage retrieval in open QA, ad-hoc information retrieval.\n",
    "\n",
    "> Use `\"query: \"` prefix for symmetric tasks such as semantic similarity, bitext mining, paraphrase retrieval.\n",
    "\n",
    "> Use `\"query: \"` prefix if you want to use embeddings as features, such as linear probing classification, clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.prefix = \"passage: \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Let's put everything together in one simple abstraction - `Indexer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from justatom.running.indexer import API as IndexerAPI\n",
    "\n",
    "# 1. \"embedding\" is the way to index the given ANN store (weaviate)\n",
    "# 2. runner is responsible for mapping docs to embeddings\n",
    "# 3. processor is responsible for tokenizing given chunks\n",
    "# 4. device - compute everything on selected `device`\n",
    "\n",
    "ix_runner = IndexerAPI.named(\"embedding\", runner=runner, store=store, processor=processor, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async for js_batch_docs in ix_runner.index(js_docs, n=32):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❗️According to the <a href=\"https://arxiv.org/abs/2212.03533\">paper</a> E5 family is trained in assymetric way meaning we have to set `prefix` back to `query: `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.prefix = \"query: \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\n",
    "    \"thinking about 'The Hunger Games' mechanics, if you were in the same shoes as Gale, entering your name forty-two times to feed your fam, how would you strategize your game in the actual Arena? Would you team up or go solo based on these high stakes?\",\n",
    "    \"In the universe of 'The Hunger Games', what are tesserae and what do they offer to the participants in the Harvest?\",\n",
    "    \"In 'Harry Potter and the Philosopher's Stone', what misconception had Harry and Hermione initially had about Snape's intentions before learning the truth?\",\n",
    "    \"Hey peeps, why is Harry all jittery and pacing around the room even after telling Hermione about the whole Snape and Voldemort situation?\",\n",
    "    \"Hey fellow gamers, in The Hunger Games universe, if you were in a match where your ally was taken down first like Rue, how would you strategize your next move to survive against top opponents like Cato?\",\n",
    "    \"In the 'Hunger Games' novel, why does Cato decide to spare Katniss's life after their encounter?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from justatom.running.retriever import API as RetrieverApi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pure keywords search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = RetrieverApi.named(\"keywords\", store=store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pos, query in enumerate(queries):\n",
    "    print(retriever.retrieve_topk(query, top_k=1)[0])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Search by embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = RetrieverApi.named(\"embedding\", store=store, runner=runner, processor=processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pos, query in enumerate(queries):\n",
    "    print(retriever.retrieve_topk(query, top_k=1)[0])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Search by embedding AND keywords\n",
    "> ❓How do we combine them? First, introduce a parameter called `alpha`, which can be any value from 0.0 to 1.0. \n",
    "\n",
    "> When `alpha = 0.0`, the search relies entirely on keywords (pure keyword search). \n",
    "\n",
    "> When `alpha = 1.0`, it uses only semantic embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = RetrieverApi.named(\"hybrid\", store=store, processor=processor, runner=runner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pos, query in enumerate(queries):\n",
    "    print(retriever.retrieve_topk(query, top_k=1, alpha=0.78)[0])\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "optimus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
